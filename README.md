# Scrapiee - FastAPI Web Scraping Service

A modern, high-performance web scraping API built with FastAPI and Camoufox, designed for extracting product data from e-commerce websites.

## ğŸš€ Features

- **âš¡ FastAPI Framework**: Modern, fast, with automatic API documentation
- **ğŸ¦Š Camoufox Integration**: Python-native anti-detection browser automation
- **ğŸ” Secure Authentication**: API key authentication with rate limiting
- **ğŸ¯ Smart Data Extraction**: Intelligent element detection for product information
- **ğŸ“Š Structured Responses**: Clean JSON with comprehensive metadata
- **â˜ï¸ Cloud Ready**: Optimized for Render.com deployment
- **ğŸ“– Auto Documentation**: Interactive API docs at `/docs`

## ğŸ“‹ API Reference

### Authentication
All requests require a Bearer token in the `Authorization` header:
```
Authorization: Bearer your-api-key-here
```

### Endpoints

#### `POST /api/scrape`
Extract product data from a URL.

**Request:**
```json
{
  "url": "https://example.com/product",
  "timeout": 30000,
  "wait_for": "networkidle"
}
```

**Response:**
```json
{
  "success": true,
  "data": {
    "title": "Product Name",
    "price": "29.99",
    "currency": "USD", 
    "description": "Product description...",
    "image": "https://example.com/image.jpg",
    "url": "https://example.com/product"
  },
  "metadata": {
    "timestamp": 1642608000,
    "processing_time": 3421,
    "extraction_method": "smart-selectors"
  }
}
```

#### `GET /health`
Service health check.

#### `GET /api/browser/status`
Browser service status (authenticated).

#### `POST /api/browser/restart`
Restart browser service (authenticated).

## ğŸ› ï¸ Local Development

### Prerequisites
- Python 3.11+
- pip or poetry

### Setup
```bash
# Clone repository
git clone <repository-url>
cd scrapiee

# Install dependencies
pip install -r requirements.txt

# Set up environment
cp .env.example .env
# Edit .env with your API key

# Start development server
python main.py
```

The API will be available at `http://localhost:8000` with interactive documentation at `/docs`.

### Project Structure
```
scrapiee/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py              # Pydantic models for requests/responses
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ browser_service.py # Camoufox browser management
â”‚       â”œâ”€â”€ extractor_service.py # Data extraction logic
â”‚       â””â”€â”€ scraper_service.py # Main scraping orchestration
â”œâ”€â”€ main.py                    # FastAPI application entry point
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ Dockerfile                 # Docker configuration
â”œâ”€â”€ render.yaml               # Render.com deployment config
â”œâ”€â”€ .env                      # Environment variables (local)
â”œâ”€â”€ .gitignore               # Git ignore patterns
â””â”€â”€ README.md                # Project documentation
```

## ğŸ“¡ Usage Examples

### cURL
```bash
curl -X POST http://localhost:8000/api/scrape \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{
    "url": "https://www.johnlewis.com/ninja-af180-max-pro-air-fryer-black/p111501862",
    "timeout": 30000,
    "wait_for": "networkidle"
  }'
```

### Python
```python
import httpx

response = httpx.post(
    "http://localhost:8000/api/scrape",
    headers={"Authorization": "Bearer your-api-key"},
    json={
        "url": "https://example.com/product",
        "timeout": 30000,
        "wait_for": "networkidle"
    }
)
data = response.json()
```

### JavaScript
```javascript
const response = await fetch('http://localhost:8000/api/scrape', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer your-api-key'
  },
  body: JSON.stringify({
    url: 'https://example.com/product',
    timeout: 30000,
    wait_for: 'networkidle'
  })
});
const data = await response.json();
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Application                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Authentication â”‚    â”‚        Rate Limiting             â”‚ â”‚
â”‚  â”‚  & Validation   â”‚    â”‚        & CORS                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Scraper Service â”‚    â”‚      Browser Service             â”‚ â”‚
â”‚  â”‚                 â”‚    â”‚                                  â”‚ â”‚
â”‚  â”‚ â€¢ Request       â”‚    â”‚ â€¢ Camoufox Management           â”‚ â”‚
â”‚  â”‚   Orchestration â”‚    â”‚ â€¢ Page Lifecycle                â”‚ â”‚
â”‚  â”‚ â€¢ Error         â”‚    â”‚ â€¢ Resource Optimization         â”‚ â”‚
â”‚  â”‚   Handling      â”‚    â”‚ â€¢ Concurrent Request Limiting   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              Extractor Service                          â”‚ â”‚
â”‚  â”‚                                                         â”‚ â”‚
â”‚  â”‚ â€¢ Smart Element Detection                               â”‚ â”‚
â”‚  â”‚ â€¢ Currency Recognition                                  â”‚ â”‚
â”‚  â”‚ â€¢ Data Cleaning & Validation                           â”‚ â”‚
â”‚  â”‚ â€¢ BeautifulSoup HTML Parsing                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Deployment

### Render.com
1. Connect your GitHub repository
2. The `render.yaml` will automatically configure deployment
3. API key will be auto-generated
4. Service will be available at `https://your-app.onrender.com`

### Docker
```bash
# Build image
docker build -t scrapiee .

# Run container
docker run -p 8000:8000 -e SCRAPER_API_KEY=your-key scrapiee
```

## âš™ï¸ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `SCRAPER_API_KEY` | - | API authentication key |
| `PORT` | 8000 | Server port |
| `ENVIRONMENT` | development | Environment mode |
| `BROWSER_TIMEOUT` | 30000 | Browser timeout (ms) |
| `MAX_CONCURRENT_REQUESTS` | 2 | Max concurrent scraping |
| `RATE_LIMIT_REQUESTS` | 10 | Requests per time window |
| `RATE_LIMIT_WINDOW` | 60 | Rate limit window (seconds) |

### Request Parameters

- `url` (required): Target URL to scrape
- `timeout` (optional): Request timeout (1000-60000ms, default: 30000)
- `wait_for` (optional): Page load condition
  - `networkidle`: Wait for network activity to stop (default)
  - `load`: Wait for load event
  - `domcontentloaded`: Wait for DOM to be ready

## ğŸ” Data Extraction

The service uses intelligent CSS selectors to extract:

- **Title**: Product titles, headings, meta titles
- **Price**: Current prices (excludes strikethrough/original prices) 
- **Description**: Product descriptions, meta descriptions
- **Image**: Product images, hero images, gallery images
- **Currency**: Auto-detected from symbols, codes, domain TLD

## ğŸ›¡ï¸ Security & Rate Limiting

- **Authentication**: Bearer token required for all scraping endpoints
- **Rate Limiting**: 10 requests per minute per IP (configurable)
- **CORS**: Configurable origins for web applications
- **Input Validation**: Pydantic models with comprehensive validation
- **Error Handling**: Secure error responses without internal details

## ğŸ“Š Performance

- **Concurrent Requests**: Limited to 2 simultaneous scraping operations
- **Resource Optimization**: Images/CSS/fonts blocked for faster loading
- **Smart Timeouts**: Fallback loading strategies for difficult sites
- **Memory Management**: Automatic page cleanup and browser restart

## ğŸ› Error Handling

Comprehensive error classification:
- `TIMEOUT`: Page load timeout
- `DNS_ERROR`: Domain resolution failure  
- `CONNECTION_REFUSED`: Server connection refused
- `INVALID_URL`: Malformed URL
- `BROWSER_ERROR`: Browser service issues
- `SCRAPING_FAILED`: General extraction failure

## ğŸ“œ License

MIT License - see LICENSE file for details.